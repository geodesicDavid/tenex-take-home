# Story 3.1: Backend LLM Integration

## Status
Draft

## Story
**As a** developer,
**I want** to integrate the backend chat service with the Google Gemini LLM,
**so that** the agent can generate intelligent, context-aware responses based on the user's calendar.

## Acceptance Criteria
1. The `/api/chat` endpoint is updated to make a call to the Google Gemini API.
2. The user's incoming message and their fetched calendar data are combined into a structured, effective prompt for the LLM.
3. The backend successfully receives the response from the LLM.
4. The backend is configured to stream the LLM's response back to the frontend client in real-time.

## Tasks / Subtasks

### LLM Integration Setup
- [ ] Configure Google Gemini API client (AC: 1)
  - [ ] Install Google Generative AI SDK
  - [ ] Set up API key management and environment variables
  - [ ] Initialize Gemini client with proper configuration
  - [ ] Add error handling for API authentication and rate limits
- [ ] Implement LLM service layer (AC: 1, 2)
  - [ ] Create `LLMService` class for Gemini integration
  - [ ] Implement prompt construction with calendar context
  - [ ] Add temperature and generation parameters
  - [ ] Handle API timeouts and retry logic

### Calendar Context Integration
- [ ] Fetch calendar data for context (AC: 2)
  - [ ] Integrate with existing CalendarService from Story 2.1
  - [ ] Fetch relevant calendar events based on user message
  - [ ] Format calendar data for LLM prompt inclusion
  - [ ] Cache calendar data to minimize API calls
- [ ] Build structured prompts (AC: 2)
  - [ ] Create prompt template with system instructions
  - [ ] Include calendar events as context in prompts
  - [ ] Format user messages appropriately for LLM
  - [ ] Add conversation history context

### Response Streaming
- [ ] Implement streaming response (AC: 3, 4)
  - [ ] Update `/api/chat` endpoint to return streaming response
  - [ ] Configure Gemini SDK for streaming responses
  - [ ] Handle chunked response encoding and headers
  - [ ] Add proper CORS headers for streaming
- [ ] Handle streaming errors and timeouts (AC: 4)
  - [ ] Implement connection timeout handling
  - [ ] Add graceful degradation for streaming failures
  - [ ] Log streaming errors for debugging
  - [ ] Fallback to non-streaming response if needed

## Dev Notes

### Previous Story Insights
This story depends on multiple previous stories:
- Story 2.4 (Frontend Chat Interface & Backend Hook) provides the existing `/api/chat` endpoint to enhance
- Story 2.1 (Backend Calendar API Endpoint) provides calendar data access
- Story 1.2 (Backend Google OAuth 2.0 Flow) provides authentication context
[Source: docs/stories/2.4.frontend-chat-interface-backend-hook.story.md, docs/stories/2.1.backend-calendar-api-endpoint.story.md, docs/stories/1.2.backend-google-oauth-flow.story.md]

### Data Models
Enhanced chat request with context [Source: architecture/data-models.md]:
```typescript
interface ChatRequestWithContext {
  message: string;
  timestamp: Date;
  calendarContext?: CalendarEvent[];
  conversationHistory?: ChatMessage[];
}
```

Streaming response format (new for this story):
```typescript
interface StreamingChatResponse {
  id: string;
  chunks: AsyncIterable<string>;
  timestamp: Date;
}
```

### API Specifications
Enhanced ChatService endpoint [Source: architecture/components.md#Backend-ChatService]:
- **Responsibility**: Takes user input, combines with calendar data, interacts with Google Gemini LLM
- **Key Interface**: Exposes the `/chat` endpoint with streaming support
- **Dependencies**: Google Gemini API, Backend: CalendarService, Google Cloud Secret Manager
- **Current Implementation**: Full LLM integration with streaming responses

LLM prompt structure [Source: architecture/components.md#Backend-ChatService]:
```
System: You are a helpful calendar assistant. You have access to the user's calendar events and can help them schedule, reschedule, or understand their schedule.

Context: [formatted calendar events]

User: [user message]

Assistant: [LLM response]
```

### Component Specifications
Backend: Enhanced ChatService [Source: architecture/components.md#Backend-ChatService]:
- **Responsibility**: Takes user input, combines with calendar data, interacts with Google Gemini LLM
- **Key Interfaces**: `/chat` endpoint (streaming), LLM service integration
- **Dependencies**: Google Gemini API, Backend: CalendarService, Google Cloud Secret Manager
- **Technology Stack**: FastAPI, Google Generative AI SDK

New: LLMService [Source: architecture/components.md#Backend-LLMService]:
- **Responsibility**: Handles communication with Google Gemini LLM
- **Key Interfaces**: Prompt construction, API calls, response streaming
- **Dependencies**: Google Gemini API
- **Technology Stack**: FastAPI, Google Generative AI SDK

### File Locations
Based on unified project structure [Source: architecture/unified-project-structure.md]:
- Enhanced chat endpoint: `apps/api/app/api/chat.py` (modify existing)
- LLM service: `apps/api/app/services/llm_service.py` (new)
- Calendar service integration: `apps/api/app/services/calendar_service.py` (enhance)
- Prompt templates: `apps/api/app/prompts/calendar_assistant.py` (new)
- Streaming utilities: `apps/api/app/utils/streaming.py` (new)
- Environment config: `apps/api/.env.example` (update)
- Types: `packages/shared/src/chat.ts` (enhance)

### Testing Requirements
Based on testing strategy [Source: architecture/testing-strategy.md]:
- Backend Tests: `apps/api/tests/` (Pytest)
- Required tests:
  - LLM service integration tests (mocked)
  - Calendar context integration tests
  - Streaming response functionality tests
  - Prompt construction validation tests
  - Error handling and timeout tests
  - API endpoint authentication tests
  - Rate limiting and retry logic tests

### Technical Constraints
- Must use Google Gemini API for LLM integration [Source: architecture/tech-stack.md]
- Must implement streaming responses [Source: epic requirements]
- Must use existing authentication middleware [Source: docs/stories/1.2.backend-google-oauth-flow.story.md]
- Must integrate with existing CalendarService [Source: docs/stories/2.1.backend-calendar-api-endpoint.story.md]
- Must handle API rate limits and timeouts gracefully [Source: architecture/backend-architecture.md#Error-Handling]
- Must include calendar context in prompts [Source: epic requirements]
- Must use proper error handling and logging [Source: architecture/coding-standards.md#Error-Handling]
- Must implement fallback mechanisms for API failures [Source: architecture/backend-architecture.md#Resilience-Patterns]

## Testing
- Backend test location: `apps/api/tests/`
- Test standards: Follow testing patterns with Pytest
- Specific requirements:
  - Mock Google Gemini API responses for testing
  - Test calendar context integration and formatting
  - Test streaming response handling and chunking
  - Test error scenarios (API failures, timeouts, rate limits)
  - Test prompt construction and validation
  - Test authentication and authorization for enhanced endpoint
  - Test integration with existing CalendarService

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-02 | 1.0 | Initial story draft | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
# Story 3.1: Backend LLM Integration

## Status

Ready for Review

## Story

**As a** developer,
**I want** to integrate the backend chat service with the Google Gemini LLM,
**so that** the agent can generate intelligent, context-aware responses based on the user's calendar.

## Acceptance Criteria

1. The `/api/chat` endpoint is updated to make a call to the Google Gemini API.
2. The user's incoming message and their fetched calendar data are combined into a structured, effective prompt for the LLM.
3. The backend successfully receives the response from the LLM.
4. The backend is configured to stream the LLM's response back to the frontend client in real-time.

## Tasks / Subtasks

### LLM Integration Setup

- [x] Configure Google Gemini API client (AC: 1)
  - [x] Install Google Generative AI SDK
  - [x] Set up API key management and environment variables
  - [x] Initialize Gemini client with proper configuration
  - [x] Add error handling for API authentication and rate limits
- [x] Implement LLM service layer (AC: 1, 2)
  - [x] Create `LLMService` class for Gemini integration
  - [x] Implement prompt construction with calendar context
  - [x] Add temperature and generation parameters
  - [x] Handle API timeouts and retry logic

### Calendar Context Integration

- [x] Fetch calendar data for context (AC: 2)
  - [x] Integrate with existing CalendarService from Story 2.1
  - [x] Fetch relevant calendar events based on user message
  - [x] Format calendar data for LLM prompt inclusion
  - [x] Cache calendar data to minimize API calls
- [x] Build structured prompts (AC: 2)
  - [x] Create prompt template with system instructions
  - [x] Include calendar events as context in prompts
  - [x] Format user messages appropriately for LLM
  - [x] Add conversation history context

### Response Streaming

- [x] Implement streaming response (AC: 3, 4)
  - [x] Update `/api/chat` endpoint to return streaming response
  - [x] Configure Gemini SDK for streaming responses
  - [x] Handle chunked response encoding and headers
  - [x] Add proper CORS headers for streaming
- [x] Handle streaming errors and timeouts (AC: 4)
  - [x] Implement connection timeout handling
  - [x] Add graceful degradation for streaming failures
  - [x] Log streaming errors for debugging
  - [x] Fallback to non-streaming response if needed

## Dev Notes

### Previous Story Insights

This story depends on multiple previous stories:

- Story 2.4 (Frontend Chat Interface & Backend Hook) provides the existing `/api/chat` endpoint to enhance
- Story 2.1 (Backend Calendar API Endpoint) provides calendar data access
- Story 1.2 (Backend Google OAuth 2.0 Flow) provides authentication context
  [Source: docs/stories/2.4.frontend-chat-interface-backend-hook.story.md, docs/stories/2.1.backend-calendar-api-endpoint.story.md, docs/stories/1.2.backend-google-oauth-flow.story.md]

### Data Models

Enhanced chat request with context [Source: architecture/data-models.md]:

```typescript
interface ChatRequestWithContext {
  message: string;
  timestamp: Date;
  calendarContext?: CalendarEvent[];
  conversationHistory?: ChatMessage[];
}
```

Streaming response format (new for this story):

```typescript
interface StreamingChatResponse {
  id: string;
  chunks: AsyncIterable<string>;
  timestamp: Date;
}
```

### API Specifications

Enhanced ChatService endpoint [Source: architecture/components.md#Backend-ChatService]:

- **Responsibility**: Takes user input, combines with calendar data, interacts with Google Gemini LLM
- **Key Interface**: Exposes the `/chat` endpoint with streaming support
- **Dependencies**: Google Gemini API, Backend: CalendarService, Google Cloud Secret Manager
- **Current Implementation**: Full LLM integration with streaming responses

LLM prompt structure [Source: architecture/components.md#Backend-ChatService]:

```
System: You are a helpful calendar assistant. You have access to the user's calendar events and can help them schedule, reschedule, or understand their schedule.

Context: [formatted calendar events]

User: [user message]

Assistant: [LLM response]
```

### Component Specifications

Backend: Enhanced ChatService [Source: architecture/components.md#Backend-ChatService]:

- **Responsibility**: Takes user input, combines with calendar data, interacts with Google Gemini LLM
- **Key Interfaces**: `/chat` endpoint (streaming), LLM service integration
- **Dependencies**: Google Gemini API, Backend: CalendarService, Google Cloud Secret Manager
- **Technology Stack**: FastAPI, Google Generative AI SDK

New: LLMService [Source: architecture/components.md#Backend-LLMService]:

- **Responsibility**: Handles communication with Google Gemini LLM
- **Key Interfaces**: Prompt construction, API calls, response streaming
- **Dependencies**: Google Gemini API
- **Technology Stack**: FastAPI, Google Generative AI SDK

### File Locations

Based on unified project structure [Source: architecture/unified-project-structure.md]:

- Enhanced chat endpoint: `apps/api/app/api/chat.py` (modify existing)
- LLM service: `apps/api/app/services/llm_service.py` (new)
- Calendar service integration: `apps/api/app/services/calendar_service.py` (enhance)
- Prompt templates: `apps/api/app/prompts/calendar_assistant.py` (new)
- Streaming utilities: `apps/api/app/utils/streaming.py` (new)
- Environment config: `apps/api/.env.example` (update)
- Types: `packages/shared/src/chat.ts` (enhance)

### Testing Requirements

Based on testing strategy [Source: architecture/testing-strategy.md]:

- Backend Tests: `apps/api/tests/` (Pytest)
- Required tests:
  - LLM service integration tests (mocked)
  - Calendar context integration tests
  - Streaming response functionality tests
  - Prompt construction validation tests
  - Error handling and timeout tests
  - API endpoint authentication tests
  - Rate limiting and retry logic tests

### Technical Constraints

- Must use Google Gemini API for LLM integration [Source: architecture/tech-stack.md]
- Must implement streaming responses [Source: epic requirements]
- Must use existing authentication middleware [Source: docs/stories/1.2.backend-google-oauth-flow.story.md]
- Must integrate with existing CalendarService [Source: docs/stories/2.1.backend-calendar-api-endpoint.story.md]
- Must handle API rate limits and timeouts gracefully [Source: architecture/backend-architecture.md#Error-Handling]
- Must include calendar context in prompts [Source: epic requirements]
- Must use proper error handling and logging [Source: architecture/coding-standards.md#Error-Handling]
- Must implement fallback mechanisms for API failures [Source: architecture/backend-architecture.md#Resilience-Patterns]

## Testing

- Backend test location: `apps/api/tests/`
- Test standards: Follow testing patterns with Pytest
- Specific requirements:
  - Mock Google Gemini API responses for testing
  - Test calendar context integration and formatting
  - Test streaming response handling and chunking
  - Test error scenarios (API failures, timeouts, rate limits)
  - Test prompt construction and validation
  - Test authentication and authorization for enhanced endpoint
  - Test integration with existing CalendarService

## Change Log

| Date       | Version | Description         | Author             |
| ---------- | ------- | ------------------- | ------------------ |
| 2025-09-02 | 1.0     | Initial story draft | Bob (Scrum Master) |
| 2025-09-10 | 1.1     | Implementation completed | James (Developer) |

## Dev Agent Record

### Agent Model Used

James (Developer)

### Debug Log References

None

### Completion Notes List

1. Successfully implemented Google Gemini API integration with proper authentication and error handling
2. Created comprehensive LLM service with streaming capabilities and retry logic
3. Integrated calendar context fetching with caching for improved performance
4. Built structured prompt templates with system instructions and conversation history
5. Implemented real-time streaming responses with proper SSE formatting
6. Added comprehensive error handling for timeouts and API failures
7. Created extensive test suite covering all major functionality
8. All syntax checks passed successfully

### File List

**Modified Files:**
- `apps/api/requirements.txt` - Added google-generativeai dependency
- `apps/api/.env.example` - Added Gemini API configuration
- `apps/api/app/core/config.py` - Added Gemini API key setting
- `apps/api/app/models/calendar.py` - Updated field names for consistency
- `apps/api/app/models/chat.py` - Enhanced with streaming and context support
- `apps/api/app/services/calendar_service.py` - Updated transformation method
- `apps/api/app/services/chat_service.py` - Enhanced with LLM and calendar integration
- `apps/api/app/api/chat.py` - Added streaming endpoint
- `apps/api/tests/test_chat_service.py` - Updated tests for new functionality

**New Files:**
- `apps/api/app/services/llm_service.py` - Google Gemini LLM integration service
- `apps/api/app/prompts/calendar_assistant.py` - Prompt builder for calendar context
- `apps/api/app/utils/streaming.py` - Streaming utilities and helpers
- `apps/api/tests/test_llm_integration.py` - LLM service tests
- `apps/api/tests/test_streaming.py` - Streaming functionality tests

## QA Results
